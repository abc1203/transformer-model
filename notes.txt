To-Do:
 3. Once working, gradually migrate code from online resource to own:
    - Model: multihead attention (done), positionembedding (done), encoder (done), decoder (done) - performance isn't as good when transferred? (not sure)
    - Training: PrepareData -> LoadData (done), LRScheduler -> TrainTransformer, Translate -> TestTransformer - somehow changing n_sentence doesn't adjust data size
 4. Train the working model (own code) with 100,000+ data points - can't use english-german-both.pkl (too small)
    - need to clean to use another dataset
 5. Present the results & Conclude


Implementing Input & Output Embedding + Positional Encoding:
 1. convert sentences into int vectors (ex. ["I am bad"] => [2 0 1])]
 2. using embedding, convert the int vectors into vectors of dimension d_model
 3. perform positional encoding - this part only needs the dimensions
 4. sum the embeddings & positional encoding


Masking:
 - Padding Mask
     - certain input sequences will first be zero-padded to a specific length, before feeding into the model => Zero Padding
     - purpose of padding mask is to ensure that the added zero values will not be processed by the model
     - padding mask marks the zero values by 1 (ex. [1, 2, 3, 4, 0, 0. 0] => [0, 0, 0, 0, 1, 1, 1])
 - Lookahead Mask
     - masking used in the 1st decoder sublayer
     - purpose of lookahead mask is to make sure that the succeeding words will not be considered in the model
     - lookahead mask marks the succeeding words by 1 


Training the model:
 1. implement lrate to be used within the Adam Optimizer
 2. preprocess the WMT dataset
 3. implement loss function
 4. train the model step-by-step by updating the weights using the loss calculated


Testing the model:
 1. convert logits/softmax probabilities to sequences
 2. convert the sequences to text
 3. calculate BLUE on the derived text

